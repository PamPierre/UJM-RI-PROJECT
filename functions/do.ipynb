{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de54399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_process import * #==> issues on the import of splitDocs, don't read efficiently the content\n",
    "## Fonction de traintement du texte\n",
    "from text_process import *\n",
    "from weiting_function import *\n",
    "from runs_function import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddb2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file = list_file_data(\"../Practice_05_data/XML-Coll-withSem_/\")\n",
    "dataset = \"../Practice_05_data/XML-Coll-withSem_/\"\n",
    "# Cell to initialise global variable\n",
    "team_name = \"DjibrilMohamedOmaimaDouae\"\n",
    "assets = \"assets/\"\n",
    "k = 1.2\n",
    "b = 0.75\n",
    "use_stem = False\n",
    "use_stopword = False\n",
    "run_id=0\n",
    "list_termes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "081975a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fileName = '612.xml'\n",
    "filePath = dataset+fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7259ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Practice_05_data/XML-Coll-withSem_/612.xml\n"
     ]
    }
   ],
   "source": [
    "content = preprocesFile(dataset+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c45d1674",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-302-3eae58a6c59e>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-302-3eae58a6c59e>\"\u001B[1;36m, line \u001B[1;32m8\u001B[0m\n\u001B[1;33m    corpus['bdy'] = {['p_1': \"ddd\",'p_2':\"ffff\"],\u001B[0m\n\u001B[1;37m                           ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "leaf_1 = 'header>title'\n",
    "leaf_2 = 'bdy>sec>ss1>p'\n",
    "leaf_2_1 = 'bdy>p'\n",
    "leaf_2_2 = 'bdy>sec'\n",
    "\n",
    "corpus = {}\n",
    "corpus['header'] = {'title':\"ddd\"}\n",
    "corpus['bdy'] = {['p_1': \"ddd\",'p_2':\"ffff\"],\n",
    "                 ['sec_1' :{['p_1': \"ddd\",'p_2':\"ffff\"],\n",
    "                            ['ss1':['p_1': \"ddd\",'p_2':\"ffff\"]]}]}\n",
    "\n",
    "p_bdy = ['p_1': \"ddd\",'p_2':\"ffff\"]\n",
    "s_bdy = {'ss1': {\n",
    "    'p_ss' : list('des paragraphe_sous_section'),\n",
    "    'text_ss' : \"content\"\n",
    "},\n",
    "        'text_sec':'content',\n",
    "        'p_sec' : list('des paragraphe_section')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16207bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1>c2>c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fa5b7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = 'article>'\n",
    "leaf_2_dict = {}\n",
    "bdy = BeautifulSoup(content,'xml').select('article>bdy')\n",
    "p = bdy[0].select('bdy>p')\n",
    "sec = bdy[0].select('bdy>sec')\n",
    "\n",
    "r = str(bdy[0]).replace(str(p[0]),\"\")\n",
    "\n",
    "section = {}\n",
    "sous_section ={} \n",
    "for i in range(len(sec)):\n",
    "    r = r.replace(str(sec[i]),\"\") # On retire toute les balise de body\n",
    "    # recuperation des sous sections de section\n",
    "    section[str('sec_{}').format(i+1)] = BeautifulSoup(content,'xml').select('article>bdy>sec>ss1')\n",
    "    # recuperation des paragraphe de section \n",
    "    section[str('p_{}').format(i+1)] = BeautifulSoup(content,'xml').select('article>bdy>sec>p')\n",
    "    \n",
    "    # recuperation les paragraphes des sous sections\n",
    "    for j in range(len(section[str('sec_{}').format(i+1)])):\n",
    "        #print(section[str('sec_{}').format(i+1)][i])\n",
    "        sous_section[str('ss_{}').format(j+1)] = BeautifulSoup(str(section[str('sec_{}').format(i+1)][j]),'xml').select('ss1>p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f8f4bb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>\n",
       "<list>\n",
       "<entry level=\"1\" type=\"bullet\">\n",
       "\n",
       " For three numbers, add them and divide them by 3:</entry>\n",
       "</list>\n",
       "</p>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sous_section['ss_1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "16d4a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdy = BeautifulSoup(content,'xml').select('article>bdy')\n",
    "p = bdy[0].select('bdy>p')\n",
    "sec = bdy[0].select('bdy>sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1d3fa194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bdy>\\n\\nIn <link xlink:href=\"../831/18831.xml\" xlink:type=\"simple\">\\nmathematics</link> and <link xlink:href=\"../685/26685.xml\" xlink:type=\"simple\">\\nstatistics</link>, the <b>arithmetic <link xlink:href=\"../192/19192.xml\" xlink:type=\"simple\">\\nmean</link></b> (or simply the <b>mean</b>) of a list of numbers is the sum of all the members of the list divided by the number of items in the list.  If the list is a <link xlink:href=\"../585/27585.xml\" xlink:type=\"simple\">\\nstatistical population</link>, then the mean of that population is called a <b>population mean</b>. If the list is a <link xlink:href=\"../361/160361.xml\" xlink:type=\"simple\">\\nstatistical sample</link>, we call the resulting <link xlink:href=\"../703/26703.xml\" xlink:type=\"simple\">\\nstatistic</link> a <b>sample mean</b>.\\n\\n\\n\\n\\n\\n</bdy>'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for s in sec:\n",
    "    \n",
    "bdy = r\n",
    "bdy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5310875d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526715e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0b69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_text_minings_full(list_terms = dict(), use_stem=bool(), use_stopword=bool()):\n",
    "    start = time.time()\n",
    "    new_list_terms = {}\n",
    "    posting_list = {}\n",
    "    dl = list()\n",
    "    i = 0\n",
    "    for docno, content in list_terms.items():\n",
    "        i+=1\n",
    "        text_clean = clean(content, use_stem, use_stopword)\n",
    "        new_list_terms[docno] = text_clean.split()\n",
    "        lt = text_clean.split()\n",
    "        current_dico = countWord(lt)\n",
    "        #sec_1 = \"[word : fre]\"\n",
    "        posting_list = countWordIntoDocs(current_dico,docno, posting_list)\n",
    "    \n",
    "    return posting_list, new_list_terms,(time.time() - start)\n",
    "\n",
    "def xml_text_minings(list_terms = dict(), use_stem=bool(), use_stopword=bool()):\n",
    "    start = time.time()\n",
    "    new_list_terms = {}\n",
    "    posting_list = {}\n",
    "    posting_list_el = {}\n",
    "    lt_ = dict()\n",
    "    lt_final = dict()\n",
    "    dl = list()\n",
    "    i = 0\n",
    "\n",
    "    for docno, content in list_terms.items():\n",
    "        i+=1\n",
    "        j = 0\n",
    "        #print(i,\"==>\",id, \"len sec==>\", len(content))\n",
    "        # if content[0] = 1 else normal indexion\n",
    "        posting_list_el.clear()\n",
    "        lt_.clear()\n",
    "        for text in content:\n",
    "            j+=1\n",
    "            soup = BeautifulSoup(str(text), \"xml\")\n",
    "            text_content = soup.get_text()\n",
    "            #print(text_content)\n",
    "            text_clean = clean(text_content, use_stem, use_stopword)\n",
    "            #new_list_terms[id] = text_clean.split()\n",
    "            lt = text_clean.split()\n",
    "            lt_[j] = lt\n",
    "            current_dico = countWord(lt)\n",
    "            #sec_1 = \"[word : fre]\"\n",
    "            posting_list_el = countWordIntoDocs(current_dico,\"sec_{}\".format(j), posting_list_el)\n",
    "            #posting_list.setdefault(docno,[]).append(countWordIntoDocs(current_dico,\"sec_{}\".format(j), posting_list_el))\n",
    "            #posting_list_el = countWordIntoDocs(current_dico,\"sec_{}\".format(j), posting_list_el) \n",
    "            #print(\"Sec[{}]==>{}\".format(j,text_content))\n",
    "            #print(\"**********-/*-*/-*-************************************************-/-*/-/-*/-*/-\",j)\n",
    "            \n",
    "        lt_final[docno] = lt_.copy()\n",
    "        posting_list[docno] = posting_list_el.copy();\n",
    "     \n",
    "        #break\n",
    "        if(i==80): break\n",
    "    return posting_list,lt_final,(time.time() - start)\n",
    "\n",
    "def splitDocs3(fileDoc, list_terme):\n",
    "    content = []\n",
    "    sec_dict = {} # keys = {chiffre incrementale} # values = {contenues des paragraphes}\n",
    "      # Read the XML file\n",
    "    with open(fileDoc, \"r\",encoding='utf-8') as file:\n",
    "      # Read each line in the file, readlines() returns a list of lines\n",
    "      content = file.readlines()\n",
    "      # Combine the lines in the list into a string\n",
    "      content = \" \".join(content)\n",
    "      soup = BeautifulSoup(content, \"xml\")\n",
    "      text =  soup.get_text()\n",
    "      id = str(soup.find_all('id')[0]).strip('</id>')\n",
    "      sec = soup.find_all('sec')\n",
    "      # list_terme[id] = ['P'|'S'] # 0==> il n'ya pas de section et le 1 il y a des sections\n",
    "   \n",
    "      list_terme[id]=sec  # Pour chaque article on recuper les sections\n",
    "    return list_terme\n",
    "\n",
    "# For all weigting function\n",
    "def all_score_wf(query_list, ltn,ltc,bm25):\n",
    "    start = time.time()\n",
    "    score_ltn= score(query,ltn)\n",
    "    print(\"Ltn time : \",time.time() -start)\n",
    "    start = time.time()\n",
    "    score_ltc = score(query,ltc)\n",
    "    print(\"Ltc time : \",time.time() -start)\n",
    "    start = time.time()\n",
    "    score_bm25 = score(query,bm25)\n",
    "    print(\"Bm25 time : \",time.time() -start)\n",
    "    \n",
    "    return score_ltn,score_ltc,score_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5406e4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02127832571665446\n"
     ]
    }
   ],
   "source": [
    "# For full content\n",
    "for file_name in list_file:\n",
    "    list_termes = splitDocs2(dataset+file_name,list_termes)\n",
    "pl_f,lt_f,ti = xml_text_minings_full(list_termes,use_stem, use_stopword)\n",
    "\n",
    "print(ti/60)\n",
    "stat_ = get_statistics(pl_f, lt_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656bd819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07944340705871582\n"
     ]
    }
   ],
   "source": [
    "# For content with sec\n",
    "for file_name in list_file:\n",
    "    list_termes = splitDocs3(dataset+file_name,list_termes)\n",
    "pl,lt,ti = xml_text_minings(list_termes,use_stem, use_stopword)\n",
    "\n",
    "new_pl = {}\n",
    "for dic, values in pl.items():\n",
    "        for word,v in values.items():\n",
    "            new_pl.setdefault(word,[]).append({dic:v})\n",
    "\n",
    "print(ti/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef44cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part of statistique\n",
    "\n",
    "def doc_len_xml(list_terms):\n",
    "    dl = {}\n",
    "    dl_sec = {}\n",
    "    n_sec = 0\n",
    "    for doc_n, sec in list_terms.items():\n",
    "        dl_sec.clear()\n",
    "        n_sec+=len(sec)\n",
    "        for sec_n, len_sec in sec.items():\n",
    "            dl_sec[sec_n] = len(len_sec)\n",
    "        dl[doc_n] = [dl_sec.copy()]\n",
    "    # dl = [(doc_n, len(len_doc)) for doc_n,len_doc in list_terms.items()]\n",
    "    return dl,n_sec\n",
    "#{docn : {sec_1:5}}\n",
    "\n",
    "def collection_term_freq_xml(posting_list):\n",
    "    c_size = {}\n",
    "    dl = {}\n",
    "    for term, article in posting_list.items():  # get the term\n",
    "        somme = 0\n",
    "        j = 0\n",
    "        for secs in article:\n",
    "            \n",
    "            for sec,val in secs.items():\n",
    "                \n",
    "                for v in val:\n",
    "                    j+=1\n",
    "                    somme += v[1]\n",
    "        dl[term] = [(somme, len(article),j)]\n",
    "                                                   # somme = somme de frequence dans toutes les sections\n",
    "    return dl                                              \n",
    "                                                            # j = nombre total d'element\n",
    "def get_xml_stat(posting_list, list_terms):\n",
    "    stat = {}\n",
    "    # document length\n",
    "    stat['df'],stat['n_sec'] = doc_len_xml(list_terms)\n",
    "    # Number of a doc\n",
    "    stat['n_article'] = len(stat['df']) # n article\n",
    "    stat['n_element'] = stat['n_sec'] + stat['n_article']\n",
    "    # term length\n",
    "    stat['tl'] = term_len(posting_list)\n",
    "    # vocabulary size\n",
    "    stat['voc_size'] = vocabulary_size(posting_list)\n",
    "    # collection frequency of terms\n",
    "    stat['colec_freq'] = collection_term_freq_xml(posting_list)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf01522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = get_xml_stat(new_pl,lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b19f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd5825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086627d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62906c7e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Article > Sections > Paragraphes\n",
    "        >Paragraphes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eca24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of weigting function\n",
    "def smart_ltn_xml(posting_list,stat):\n",
    "    i = 0\n",
    "    n = stat['n_element']\n",
    "    df_t = stat['colec_freq']\n",
    "    df = 0\n",
    "    #(1+LOG(tf))*LOG(n/df)\n",
    "    #tf is the frequence of term on a document\n",
    "    #df number of document where the term appear\n",
    "    ltn_value_sec = {} # key sec, value (terme, ltn)\n",
    "    ltn_value = {} # key doc number, value (terme, ltn)\n",
    "    for term, articles in posting_list.items():\n",
    "        i+=1\n",
    "        dl = df_t[term]\n",
    "        df = dl[0][2] # nombre de doc dans lequel le terme apparait sur toute la collection\n",
    "        # print(value) [{'10013': [('sec_1', 1), ('sec_12', 1)]},...]\n",
    "        \n",
    "        for sec in articles:\n",
    "            \n",
    "            for doc_no,val in sec.items():\n",
    "                ltn_value_sec.clear()\n",
    "                for v in val:\n",
    "                    tf = v[1]\n",
    "                    sec =v[0]\n",
    "                    ltn = (1+math.log10(tf))*(math.log10(n/df))\n",
    "                    ltn_value_sec.setdefault(sec,[]).append((term,round(ltn,4))) # ltn en fonction de la section\n",
    "                ltn_value.setdefault(doc_no,[]).append(ltn_value_sec.copy()) # ltn en fonction des articles\n",
    "    return ltn_value\n",
    "\n",
    "def ltn_ref(ltn):\n",
    "    new_ltn = {}\n",
    "    ltn_refractor = {}\n",
    "    for doc, article in ltn.items():\n",
    "        new_ltn.clear()\n",
    "        for l in article: \n",
    "            for se, k in l.items():\n",
    "                for i in k:\n",
    "                    new_ltn.setdefault(se,[]).append(i)\n",
    "        ltn_refractor[doc] = new_ltn.copy()\n",
    "    return ltn_refractor\n",
    "\n",
    "def xml_w_score(wf=dict()):\n",
    "    score_wt = {}\n",
    "    for doc_no, article in wf.items():\n",
    "        scores = rsv_score(query,article)\n",
    "        if(len(scores)>0):\n",
    "            for sec in scores:\n",
    "                score_wt.setdefault(doc_no,[]).append(sec)\n",
    "    return score_wt\n",
    "\n",
    "def ltc_xml(ltn):\n",
    "    score_ltn = {}\n",
    "    for doc_no, sec_ltn_value in ltn.items():\n",
    "        for value in sec_ltn_value:\n",
    "            res = smart_ltc(sec_ltn_value)\n",
    "            score_ltn[doc_no] = res\n",
    "    return score_ltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea4b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat['n_element']\n",
    "ltn = ltn_ref(smart_ltn_xml(new_pl,stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6df555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltn_,ltc_,bm25_ =weinting_function(pl_f, stat_,k,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d26893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94413802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltc =ltc_xml(ltn)\n",
    "#score_ltc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d6182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7049e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_run_file_xml(run_id, wf_score, index, use_stem, use_stopword, k, b):\n",
    "    start = time.time()\n",
    "    if index != 2:\n",
    "        k = 0.0\n",
    "        b = 0.0\n",
    "    run_file_name, run_id = create_run_file(run_id, index, use_stem, use_stopword, k, b)\n",
    "    for article, scores in wf_score.items():\n",
    "        for score in scores:\n",
    "            print(len(score))\n",
    "            for i in range(len(score)):\n",
    "                run_score = str(score[i]).replace(',', '').replace(\"'\", '').replace('(', '').replace(')', '')\n",
    "                print(run_score)\n",
    "                break\n",
    "                run_file_name.write(run_score + \"\\n\")\n",
    "        run_file_name.close()\n",
    "        break\n",
    "    print(\"Execution time for the run {} is {}\".format(run_id, time.time() - start))\n",
    "    return run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10bbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6fe16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scores(query_list,rsv_wf):\n",
    "    team_name = \"DjibrilMohamedOmaimaDouae\"\n",
    "    score = []\n",
    "    for query in query_list:\n",
    "        query = query.split(' ', 1)\n",
    "        result = result_querys(query[0],rsv_score_(query[1], rsv_wf), team_name)\n",
    "        score.append(result)\n",
    "    return score\n",
    "\n",
    "def result_querys(num_query,rsv_result,team_name):\n",
    "    start = time.time()\n",
    "    #score = reverse_score(rsv_result)\n",
    "    rsv_r = []\n",
    "    k = 0\n",
    "    x = rsv_result\n",
    "    if len(x)>1500:\n",
    "        x = x[:1500]\n",
    "    rsv_r = [str('{} Q0 {} {} {} {} /article[1]/sec[{}]'.format(num_query,\n",
    "                                                           x[i][2],i+1+run_id,\n",
    "                                                           round(x[i][0],5),\n",
    "                                                           team_name,\n",
    "                                                          x[i][1].replace(\"sec_\",\"\"))) \n",
    "             for i in range(len(x))]\n",
    "    return rsv_r\n",
    "\n",
    "# 2009011 Q0 1164 1 0.7262 DjibrilMohamedOmaimaDouae /article[1]/sec\n",
    "def rsv_score_(query, wf):\n",
    "    rsv_doc = {}\n",
    "    rsv_sec= {}\n",
    "    rsv = []\n",
    "    for term in query.split():\n",
    "        for doc, sec in wf.items():\n",
    "            rsv_sec.clear()\n",
    "            for k,val in sec.items():\n",
    "                for v in val:\n",
    "                    if term == v[0]:\n",
    "                        if k in rsv_sec.keys():\n",
    "                            v_temp = rsv_sec[k] + v[1]\n",
    "                            v[1] = v_temp\n",
    "                        rsv.append((v[1],k,doc))\n",
    "                    else:\n",
    "                        continue \n",
    "            if(len(rsv_sec.keys())>0):\n",
    "                rsv_doc[doc] = rsv_sec.copy()\n",
    "    rsv = sorted(rsv, key=lambda item: item[0],reverse=True)\n",
    "    return rsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a49558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Practice_04_data/topics_M2DSC_7Q.txt\n"
     ]
    }
   ],
   "source": [
    "# get the query file\n",
    "directory = \"../Practice_04_data/\"\n",
    "#list_data_q = practice2.list_file_data(directory)\n",
    "filename_q = str(directory+\"topics_M2DSC_7Q.txt\")\n",
    "query_list =preprocesFile(filename_q)\n",
    "query = query_list.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6054e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ltn time :  0.6507244110107422\n",
      "Ltc time :  0.39051151275634766\n",
      "Bm25 time :  0.39559245109558105\n"
     ]
    }
   ],
   "source": [
    "ltn_,ltc_,bm25_ =weinting_function(pl_f, stat_,k,b) \n",
    "score_ltn_,score_ltc_,score_bm25_ = all_score_wf(query, ltn_,ltc_,bm25_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "335e4445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "96\n",
      "48\n",
      "676\n",
      "241\n",
      "191\n",
      "165\n",
      "208\n",
      "Execution time for the run 1 is 0.019765138626098633\n"
     ]
    }
   ],
   "source": [
    "ltn_s = scores(query,ltn)\n",
    "run_id = build_run_file(run_id, ltn_s, 0, use_stem, use_stopword, k, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbda3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115db92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb7aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}