{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de54399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this 1\n",
    "from file_process import * #==> issues on the import of splitDocs, don't read efficiently the content\n",
    "## Fonction de traintement du texte\n",
    "from text_process import *\n",
    "from weiting_function import *\n",
    "from runs_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddb2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "list_file = list_file_data(\"../Practice_05_data/XML-Coll-withSem/\")\n",
    "dataset = \"../Practice_05_data/XML-Coll-withSem/\"\n",
    "# Cell to initialise global variable\n",
    "team_name = \"DjibrilMohamedOmaimaDouae\"\n",
    "assets = \"assets/\"\n",
    "k = 1.2\n",
    "b = 0.75\n",
    "use_stem = False\n",
    "use_stopword = True\n",
    "run_id=0\n",
    "list_termes = {}\n",
    "\n",
    "# get the query file\n",
    "directory = \"../Practice_04_data/\"\n",
    "#list_data_q = practice2.list_file_data(directory)\n",
    "filename_q = str(directory+\"topics_M2DSC_7Q.txt\")\n",
    "query_list =preprocesFile(filename_q)\n",
    "query = query_list.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081975a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fileName = '10237.xml'\n",
    "#filePath = dataset+fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7259ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#content = preprocesFile(dataset+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45d1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't run the cell\n",
    "leaf_1 = 'header>title'\n",
    "leaf_2 = 'bdy>sec>ss1>p'\n",
    "leaf_2_1 = 'bdy>p'\n",
    "leaf_2_2 = 'bdy>sec'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16207bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad11ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5b7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#leaf_2_dict = {}\n",
    "#bdy = BeautifulSoup(content,'xml').find_all('bdy')\n",
    "#p = bdy[0].select('bdy>p')\n",
    "#sec = bdy[0].select('bdy>sec')\n",
    "\n",
    "#r = str(bdy[0]).replace(str(p[0]),\"\")\n",
    "\n",
    "\n",
    "#len(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f4bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def parse_xml(content,list_id):\n",
    "    corpus = {} # Init corpus\n",
    "    corpus['title'] = BeautifulSoup(content,'xml').find_all('title')\n",
    "    bdy = BeautifulSoup(content,'xml').find_all('bdy')\n",
    "    corpus['bdy'] = {} # Init body part\n",
    "    id = str(BeautifulSoup(content,'xml').find_all('id')[0]).strip('</id>')\n",
    "    # Recuperation des paragraphes dans le body\n",
    "    p = bdy[0].select('bdy>p')\n",
    "    b = bdy[0]\n",
    "    #Recuperation des sections dans le body\n",
    "    sec = bdy[0].select('bdy>sec')\n",
    "    paragraphe = {}\n",
    "    if(len(p)>0):\n",
    "        for i in range(len(p)):\n",
    "            bdy[0] = str(bdy[0]).replace(str(p[i]),\"\") # Supprimer les paragraphes \n",
    "            paragraphe[str('p.{}').format(i+1)] = p[i] # Ajoute le i_eme paragraphe de body\n",
    "\n",
    "        corpus['bdy'].update(paragraphe) # Mise a jours de body\n",
    "            \n",
    "    if(len(sec)>0):\n",
    "        section = {}\n",
    "        sous_section ={} \n",
    "        sous_section_p ={} \n",
    "        rest = {}\n",
    "        for i in range(len(sec)):\n",
    "            section['sec.{}'.format(i+1)] = {}\n",
    "            sous_section.clear()\n",
    "            \n",
    "            rest.clear()\n",
    "            paragraphe.clear()\n",
    "            #print('sec_{}'.format(i))\n",
    "            r_p = sec[i].get_text()\n",
    "            bdy[0] = str(bdy[0]).replace(str(sec[i]),\"\") # On retire tout le contenue de sec[i] de body\n",
    "            \n",
    "            #print('ss : {}'.format(len(sous_sections)))\n",
    "            # recuperation des paragraphe de section\n",
    "            par = BeautifulSoup(str(sec[i]),'xml').select('sec>p')\n",
    "            if(len(par)>0):\n",
    "                #print('par : {}'.format(len(par)))\n",
    "                # a = r_p\n",
    "                j = 0\n",
    "                for p in par:\n",
    "                    \n",
    "                    r_p = str(r_p).replace(p.get_text(),\"\") # Recupere le texte et le supprime dans la section\n",
    "                    \n",
    "                    paragraphe[str('p.{}').format(j+1)] = p\n",
    "                    j+=1\n",
    "                section['sec.{}'.format(i+1)] = paragraphe.copy()\n",
    "                 \n",
    "            # recuperation des sous sections de section\n",
    "            sous_sections = BeautifulSoup(str(sec[i]),'xml').select('sec>ss1')\n",
    "            if(len(sous_sections)>0):\n",
    "                # recuperation les paragraphes des sous sections\n",
    "                for j in range(len(sous_sections)):\n",
    "                    sous_section_p.clear() \n",
    "                    sous_section['ss1.{}'.format(j+1)] = {}\n",
    "                    #print('\\t\\tss1')\n",
    "                    #print(str(sous_sections[j]))\n",
    "                    r_p = r_p.replace(sous_sections[j].get_text(),\"\")\n",
    "                    ss_p = BeautifulSoup(str(sous_sections[j]),'xml').select('ss1>p')\n",
    "                    if(len(ss_p)>0):\n",
    "                        #print('ss_p : {}'.format(len(ss_p)))\n",
    "                        for k in range(len(ss_p)):\n",
    "                            sous_section_p['p.{}'.format(k+1)] = ss_p[k]\n",
    "                    sous_section['ss1.{}'.format(j+1)].update(sous_section_p.copy())\n",
    "                #print(sous_section.keys())\n",
    "            if(len(sous_section.keys())>0):\n",
    "                section['sec.{}'.format(i+1)].update(sous_section.copy())\n",
    "\n",
    "            if(len(r_p)>0):\n",
    "                rest['NaN'] = r_p\n",
    "                section['sec.{}'.format(i+1)].update(rest)\n",
    "        #print(\"insertiong data on corpus\")\n",
    "        corpus['bdy'].update(section)\n",
    "        if(len(bdy[0])>0):\n",
    "            \n",
    "            #print(\"rest\",b)\n",
    "            rest['NaN'] = bdy[0]\n",
    "            corpus['bdy'].update(rest)\n",
    "    list_termes[id] = corpus\n",
    "    return list_termes\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d4a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "def build_pl(text, node,posting_list_el,use_stem, use_stopword):\n",
    "    text_clean = clean(text, use_stem, use_stopword)\n",
    "    lt = text_clean.split()\n",
    "    #lt_['{}_{}'.format(node,j)] = lt\n",
    "    current_dico = countWord(lt)\n",
    "    posting_list_el = countWordIntoDocs(current_dico,'{}'.format(node), posting_list_el)\n",
    "    return posting_list_el\n",
    "\n",
    "def xml_text_minings_v2(list_terms = dict(), use_stem=bool(), use_stopword=bool()):\n",
    "    start = time.time()\n",
    "    new_list_terms = {}\n",
    "    posting_list = {}\n",
    "    posting_list_el = {}\n",
    "    lt = dict()\n",
    "    lt_final = dict()\n",
    "    dl = list()\n",
    "    i = 0\n",
    "\n",
    "    for docno, title_bdy in list_terms.items():\n",
    "        i+=1\n",
    "        j = 0\n",
    "        lt.clear()\n",
    "        posting_list_el.clear()\n",
    "      \n",
    "        title = title_bdy['title'][0].get_text()\n",
    "        title  =clean(title, use_stem, use_stopword)\n",
    "        lt['title'] = title.split() # Recuperation de content title\n",
    "        posting_list_el = build_pl(title, \"title\",posting_list_el,use_stem, use_stopword)\n",
    "        \n",
    "        for node_1, value_1 in title_bdy[\"bdy\"].items(): \n",
    "            if \"sec\" in node_1:\n",
    "                for sec, value_2 in value_1.items():# Recuperation des sections de body\n",
    "                    if \"ss\" in sec:\n",
    "                        for p,val in value_2.items(): # Recuperation des paragraphe de chaque sous section\n",
    "                            v = clean(val.text, use_stem, use_stopword)\n",
    "                            lt[str(\"bdy_{}_{}_{}\".format(node_1,sec,p))] = v.split()\n",
    "                            posting_list_el = build_pl(v, str(\"bdy_{}_{}_{}\".format(node_1,sec,p)),posting_list_el,use_stem, use_stopword)          \n",
    "                    else:\n",
    "                        if \"NaN\" in sec: # Recuperation des NaN de la section\n",
    "                            val = value_2.strip()  \n",
    "                        else : \n",
    "                            val = clean(value_2.text, use_stem, use_stopword) # Recuperation des pa\n",
    "                        lt[str(\"bdy_{}_{}\".format(node_1,sec))] = val.split()\n",
    "                        posting_list_el = build_pl(val, str(\"bdy_{}_{}\".format(node_1,sec)),posting_list_el,use_stem, use_stopword)\n",
    " \n",
    "            else:\n",
    "                if \"NaN\" in node_1: # Recuperation des NaN de body\n",
    "                    val = value_2.strip()  \n",
    "                else : \n",
    "                    val = clean(value_1.text, use_stem, use_stopword) # Recuperation des paragraphes de body\n",
    "                lt[str(\"bdy_{}\".format(node_1))] = val.split()\n",
    "                posting_list_el = build_pl(val, str(\"bdy_{}\".format(node_1)),posting_list_el,use_stem, use_stopword)\n",
    "\n",
    "      \n",
    "        lt_final[docno] = lt.copy()\n",
    "        posting_list[docno] = posting_list_el.copy()\n",
    "\n",
    "    return posting_list,lt_final,(start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3fa194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10003934.xml\n",
      "2 10008302.xml\n",
      "3 10013.xml\n",
      "4 10013985.xml\n",
      "5 10018.xml\n",
      "6 10019528.xml\n",
      "7 10022607.xml\n",
      "8 1002307.xml\n",
      "9 10025826.xml\n",
      "10 10025863.xml\n",
      "11 10037185.xml\n",
      "12 10045226.xml\n",
      "13 10046553.xml\n",
      "14 10046651.xml\n",
      "15 1004679.xml\n",
      "16 1004743.xml\n",
      "17 10048713.xml\n",
      "18 10050395.xml\n",
      "19 10052718.xml\n",
      "20 100558.xml\n",
      "21 10056712.xml\n",
      "22 10057294.xml\n",
      "23 1005923.xml\n",
      "24 10061279.xml\n",
      "25 10061978.xml\n",
      "26 1006214.xml\n",
      "27 10063963.xml\n",
      "28 10069526.xml\n",
      "29 10072671.xml\n",
      "30 10073799.xml\n",
      "31 10074317.xml\n",
      "32 10077874.xml\n",
      "33 10078385.xml\n",
      "34 1007928.xml\n",
      "35 1008031.xml\n",
      "36 10083025.xml\n",
      "37 10085727.xml\n",
      "38 1008632.xml\n",
      "39 1008941.xml\n",
      "40 10089909.xml\n",
      "41 10092617.xml\n",
      "42 1009996.xml\n",
      "43 1010280.xml\n",
      "44 10105635.xml\n",
      "45 10105982.xml\n",
      "46 1011379.xml\n",
      "47 10118518.xml\n",
      "48 10119206.xml\n",
      "49 10123713.xml\n",
      "50 1012806.xml\n",
      "51 1013089.xml\n",
      "52 10131591.xml\n",
      "53 10132085.xml\n",
      "54 10133976.xml\n",
      "55 10135033.xml\n",
      "56 10136.xml\n",
      "57 10140499.xml\n",
      "58 10141125.xml\n",
      "59 10143509.xml\n",
      "60 10143862.xml\n",
      "61 1014399.xml\n",
      "62 10145406.xml\n",
      "63 10148645.xml\n",
      "64 10153951.xml\n",
      "65 1015600.xml\n",
      "66 1015602.xml\n",
      "67 10159567.xml\n",
      "68 10161696.xml\n",
      "69 1016345.xml\n",
      "70 10165060.xml\n",
      "71 10166138.xml\n",
      "72 10166432.xml\n",
      "73 101673.xml\n",
      "74 10167616.xml\n",
      "75 101700.xml\n",
      "76 10170539.xml\n",
      "77 10171509.xml\n",
      "78 10174473.xml\n",
      "79 1017564.xml\n",
      "80 10177217.xml\n",
      "81 10178154.xml\n",
      "82 10179411.xml\n",
      "83 10182619.xml\n",
      "84 1018396.xml\n",
      "85 101851.xml\n",
      "86 10191064.xml\n",
      "87 1019406.xml\n",
      "88 10194908.xml\n",
      "89 1020021.xml\n",
      "90 10203043.xml\n",
      "91 1020613.xml\n",
      "92 1021521.xml\n",
      "93 10217523.xml\n",
      "94 10218640.xml\n",
      "95 10219559.xml\n",
      "96 10219719.xml\n",
      "97 10222549.xml\n",
      "98 10222994.xml\n",
      "99 10226135.xml\n",
      "100 1023073.xml\n",
      "101 10233495.xml\n",
      "102 10237.xml\n",
      "103 10237096.xml\n",
      "104 1024371.xml\n",
      "105 10246907.xml\n",
      "106 10252733.xml\n",
      "107 102539.xml\n",
      "108 1025538.xml\n",
      "109 10255540.xml\n",
      "110 1025794.xml\n",
      "111 10259225.xml\n",
      "112 1025942.xml\n",
      "113 1026150.xml\n",
      "114 1026417.xml\n",
      "115 1026437.xml\n",
      "116 10264717.xml\n",
      "117 10266288.xml\n",
      "118 1027488.xml\n",
      "119 10280022.xml\n",
      "120 1028253.xml\n",
      "121 1029051.xml\n",
      "122 1029087.xml\n",
      "123 10294.xml\n",
      "124 1029426.xml\n",
      "125 10297660.xml\n",
      "126 102980.xml\n",
      "127 10298070.xml\n",
      "128 1030028.xml\n",
      "129 10304567.xml\n",
      "130 10308195.xml\n",
      "131 10309115.xml\n",
      "132 1031357.xml\n",
      "133 1031430.xml\n",
      "134 1031564.xml\n",
      "135 1031713.xml\n",
      "136 1031771.xml\n",
      "137 1032327.xml\n",
      "138 10323757.xml\n",
      "139 10328235.xml\n",
      "140 1033505.xml\n",
      "141 1033534.xml\n",
      "142 103358.xml\n",
      "143 10338164.xml\n",
      "144 1033846.xml\n",
      "145 1033877.xml\n",
      "146 1034327.xml\n",
      "147 1034458.xml\n",
      "148 10346175.xml\n",
      "149 10346388.xml\n",
      "150 1035054.xml\n",
      "151 10352756.xml\n",
      "152 103533.xml\n",
      "153 10356531.xml\n",
      "154 10358245.xml\n",
      "155 1036865.xml\n",
      "156 10371071.xml\n",
      "157 10377.xml\n",
      "158 10377597.xml\n",
      "159 1039587.xml\n",
      "160 10396762.xml\n",
      "161 10400382.xml\n",
      "162 10403320.xml\n",
      "163 10404403.xml\n",
      "164 1040512.xml\n",
      "165 1040597.xml\n",
      "166 1041204.xml\n",
      "167 10414589.xml\n",
      "168 10416781.xml\n",
      "169 10418991.xml\n",
      "170 10422285.xml\n",
      "171 10422354.xml\n",
      "172 10422415.xml\n",
      "173 1042273.xml\n",
      "174 1042932.xml\n",
      "175 10429807.xml\n",
      "176 10429990.xml\n",
      "177 10433312.xml\n",
      "178 10439522.xml\n",
      "179 10441625.xml\n",
      "180 10442539.xml\n",
      "181 104444.xml\n",
      "182 1044497.xml\n",
      "183 10448735.xml\n",
      "184 1045012.xml\n",
      "185 10457032.xml\n",
      "186 1046120.xml\n",
      "187 10461532.xml\n",
      "188 10464614.xml\n",
      "189 10468531.xml\n",
      "190 10468834.xml\n",
      "191 1047013.xml\n",
      "192 1047293.xml\n",
      "193 10474.xml\n",
      "194 10476514.xml\n",
      "195 10477221.xml\n",
      "196 10478526.xml\n",
      "197 10485477.xml\n",
      "198 10491349.xml\n",
      "199 1049666.xml\n",
      "200 1049815.xml\n",
      "0.6949771960576375\n"
     ]
    }
   ],
   "source": [
    "# this is important\n",
    "# For full content\n",
    "start = time.time()\n",
    "i = 0\n",
    "for file_name in list_file:\n",
    "    i+=1\n",
    "    print(i, file_name)\n",
    "    if i==200:\n",
    "        break\n",
    "    list_termes = parse_xml(preprocesFile(dataset+file_name),list_termes)\n",
    "print((time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe9f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for only full content indexing\n",
    "def get_xml_text(value):\n",
    "    j = 0\n",
    "    lt_ = dict()\n",
    "    lt_final = dict()\n",
    "    posting_list = {}\n",
    "    posting_list_el = {}\n",
    "    for node, content in value.items():\n",
    "        posting_list_el.clear()\n",
    "        lt_.clear()\n",
    "        for text in content:\n",
    "            j+=1\n",
    "            soup = BeautifulSoup(str(text), \"xml\")\n",
    "            text_content = soup.get_text()\n",
    "            text_clean = clean(text_content, use_stem, use_stopword)\n",
    "            lt = text_clean.split()\n",
    "            lt_['{}_{}'.format(node,j)] = lt\n",
    "            current_dico = countWord(lt)\n",
    "            posting_list_el = countWordIntoDocs(current_dico,'{}_{}'.format(node,j), posting_list_el)\n",
    "        lt_final[node] = lt_.copy()\n",
    "        posting_list[node] = posting_list_el.copy();\n",
    "    return lt_final,posting_list\n",
    "\n",
    "def xml_text_minings_full(list_terms = dict(), use_stem=bool(), use_stopword=bool()):\n",
    "    start = time.time()\n",
    "    new_list_terms = {}\n",
    "    posting_list = {}\n",
    "    dl = list()\n",
    "    i = 0\n",
    "    for docno, content in list_terms.items():\n",
    "        i+=1\n",
    "        text_clean = clean(content, use_stem, use_stopword)\n",
    "        new_list_terms[docno] = text_clean.split()\n",
    "        lt = text_clean.split()\n",
    "        current_dico = countWord(lt)\n",
    "        #sec_1 = \"[word : fre]\"\n",
    "        posting_list = countWordIntoDocs(current_dico,docno, posting_list)\n",
    "    \n",
    "    return posting_list, new_list_terms,(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dd = list_termes[\"10843\"]\n",
    "#dd['bdy']['sec.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0b69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to take only the sections\n",
    "def xml_text_minings(list_terms = dict(), use_stem=bool(), use_stopword=bool()):\n",
    "    start = time.time()\n",
    "    new_list_terms = {}\n",
    "    posting_list = {}\n",
    "    posting_list_el = {}\n",
    "    lt_ = dict()\n",
    "    lt_final = dict()\n",
    "    dl = list()\n",
    "    i = 0\n",
    "\n",
    "    for docno, content in list_terms.items():\n",
    "        i+=1\n",
    "        j = 0\n",
    "        #print(i,\"==>\",id, \"len sec==>\", len(content))\n",
    "        # if content[0] = 1 else normal indexion\n",
    "        posting_list_el.clear()\n",
    "        lt_.clear()\n",
    "        for text in content:\n",
    "            j+=1\n",
    "            soup = BeautifulSoup(str(text), \"xml\")\n",
    "            text_content = soup.get_text()\n",
    "            #print(text_content)\n",
    "            text_clean = clean(text_content, use_stem, use_stopword)\n",
    "            #new_list_terms[id] = text_clean.split()\n",
    "            lt = text_clean.split()\n",
    "            lt_[j] = lt\n",
    "            current_dico = countWord(lt)\n",
    "            #sec_1 = \"[word : fre]\"\n",
    "            posting_list_el = countWordIntoDocs(current_dico,\"sec_{}\".format(j), posting_list_el)\n",
    "            #posting_list.setdefault(docno,[]).append(countWordIntoDocs(current_dico,\"sec_{}\".format(j), posting_list_el))\n",
    "            #posting_list_el = countWordIntoDocs(current_dico,\"sec_{}\".format(j), posting_list_el) \n",
    "            #print(\"Sec[{}]==>{}\".format(j,text_content))\n",
    "            #print(\"**********-/*-*/-*-************************************************-/-*/-/-*/-*/-\",j)\n",
    "            \n",
    "        lt_final[docno] = lt_.copy()\n",
    "        posting_list[docno] = posting_list_el.copy();\n",
    "     \n",
    "        #break\n",
    "        #if(i==80): break\n",
    "    return posting_list,lt_final,(time.time() - start)\n",
    "\n",
    "def splitDocs3(fileDoc, list_terme):\n",
    "    content = []\n",
    "    sec_dict = {} # keys = {chiffre incrementale} # values = {contenues des paragraphes}\n",
    "      # Read the XML file\n",
    "    with open(fileDoc, \"r\",encoding='utf-8') as file:\n",
    "      # Read each line in the file, readlines() returns a list of lines\n",
    "      content = file.readlines()\n",
    "      # Combine the lines in the list into a string\n",
    "      content = \" \".join(content)\n",
    "      soup = BeautifulSoup(content, \"xml\")\n",
    "      text =  soup.get_text()\n",
    "      id = str(soup.find_all('id')[0]).strip('</id>')\n",
    "      sec = soup.find_all('sec')\n",
    "      # list_terme[id] = ['P'|'S'] # 0==> il n'ya pas de section et le 1 il y a des sections\n",
    "   \n",
    "      list_terme[id]=sec  # Pour chaque article on recuper les sections\n",
    "    return list_terme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9e8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Part of statistique\n",
    "\n",
    "def doc_len_xml(list_terms):\n",
    "    dl = []\n",
    "    dl_sec = {}\n",
    "    n_el = 0\n",
    "    i =0\n",
    "    title = 0\n",
    "    bdy_n = 0\n",
    "    bdy_p = 0\n",
    "    sec_n = 0\n",
    "    sec_p  =0\n",
    "    ss_p = 0\n",
    "    for doc_n, sec in list_terms.items():\n",
    "        \n",
    "        n = ''\n",
    "        i+=1\n",
    "        #if i==10: break\n",
    "            \n",
    "        for nodes, len_sec in sec.items():\n",
    "            \n",
    "            for node in nodes.split(\"_\"):\n",
    "                #print(len_sec)\n",
    "                if \"bdy\" in node: \n",
    "                    n = \"bdy\"\n",
    "                elif \"title\" in node: \n",
    "                    title+=1\n",
    "                    \n",
    "                elif(\"sec\" in node):\n",
    "                    n = \"sec\"\n",
    "                  \n",
    "                elif(\"ss\" in node):\n",
    "                    n = \"ss\"\n",
    "                    \n",
    "                if(\"p\" in node):\n",
    "                    if n==\"bdy\":\n",
    "                        bdy_p+=1\n",
    "                    elif n==\"sec\":\n",
    "                        sec_p+=1\n",
    "                    else:\n",
    "                        ss_p+=1\n",
    "                        \n",
    "                if(\"NaN\" in node):\n",
    "                    if n==\"bdy\":\n",
    "                        bdy_n+=1\n",
    "                    elif n==\"sec\":\n",
    "                        sec_n +=1\n",
    "    dl = [title,bdy_n,bdy_p,sec_n,sec_p,ss_p]\n",
    "    n_el = sum(dl)\n",
    "    # dl = [(doc_n, len(len_doc)) for doc_n,len_doc in list_terms.items()]\n",
    "    return len(list_termes.keys()), dl,(n_el+len(list_termes.keys()))\n",
    "\n",
    "#{docn : {sec_1:5}}\n",
    "def get_xml_stat(posting_list, list_terms):\n",
    "    stat = {}\n",
    "    # document length\n",
    "    # [title,bdy_n,bdy_p,sec_n,sec_p,ss_p]\n",
    "    stat['n_article'],stat['n_node'],stat['n_element'] = doc_len_xml(list_terms)\n",
    "    # Number of a doc\n",
    "    \n",
    "    # term length\n",
    "    stat['tl'] = term_len(posting_list)\n",
    "    # vocabulary size\n",
    "    stat['voc_size'] = vocabulary_size(posting_list)\n",
    "    # collection frequency of terms\n",
    "    stat['colec_freq'] = collection_term_freq_xml(posting_list)\n",
    "    return stat\n",
    "\n",
    "def collection_term_freq_xml(posting_list):\n",
    "    c_size = {}\n",
    "    dl = {}\n",
    "    for term, article in posting_list.items():  # get the term\n",
    "        somme = 0\n",
    "        j = 0\n",
    "        title = 0\n",
    "        bdy_n = 0\n",
    "        bdy_p = 0\n",
    "        sec_n = 0\n",
    "        sec_p  =0\n",
    "        ss_p = 0\n",
    "        n = ''\n",
    "        for secs in article:\n",
    "            for sec,val in secs.items():\n",
    "                for nodes in val:\n",
    "                    for node in nodes[0].split(\"_\"):\n",
    "                        \n",
    "                        if \"bdy\" in node: \n",
    "                            n = \"bdy\"\n",
    "\n",
    "                        elif \"title\" in node: \n",
    "                            title+=nodes[1]\n",
    "\n",
    "                        elif(\"sec\" in node):\n",
    "                            n = \"sec\"\n",
    "\n",
    "                        elif(\"ss\" in node):\n",
    "                            n = \"ss\"\n",
    "\n",
    "                        if(\"p\" in node):\n",
    "                            if n==\"bdy\":\n",
    "                                bdy_p+=nodes[1]\n",
    "                            elif n==\"sec\":\n",
    "                                sec_p+=nodes[1]\n",
    "                            else:\n",
    "                                ss_p+=nodes[1]\n",
    "\n",
    "                        if(\"NaN\" in node):\n",
    "                            if n==\"bdy\":\n",
    "                                bdy_n+=nodes[1]\n",
    "                            elif n==\"sec\":\n",
    "                                sec_n +=nodes[1]\n",
    "        j = sum([title,bdy_n,bdy_p,sec_n,sec_p,ss_p])\n",
    "        dl[term] = [title,bdy_n,bdy_p,sec_n,sec_p,ss_p ,j]\n",
    "                                                   # somme = somme de frequence dans toutes les sections\n",
    "    return dl                                              \n",
    "                                                            # j = nombre total d'element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For content with sec\n",
    "\"\"\"\n",
    "for file_name in list_file:\n",
    "    list_termes = splitDocs3(dataset+file_name,list_termes)\n",
    "pl,lt,ti = xml_text_minings(list_termes,use_stem, use_stopword)\n",
    "\n",
    "new_pl = {}\n",
    "for dic, values in pl.items():\n",
    "        for word,v in values.items():\n",
    "            new_pl.setdefault(word,[]).append({dic:v})\n",
    "\n",
    "print(ti/60)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35141c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Indexing time is : 0.056399397055308026\n",
      "Execution time for indexing and statistique\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "pl,lt,ti = xml_text_minings_v2(list_termes,use_stem, use_stopword)\n",
    "\n",
    "new_pl = {}\n",
    "for dic, values in pl.items():\n",
    "        for word,v in values.items():\n",
    "            new_pl.setdefault(word,[]).append({dic:v})\n",
    "print(\"....Indexing time is : {}\".format((time.time() - ti)/60))\n",
    "stat = get_xml_stat(new_pl,lt)\n",
    "print(\"Execution time for indexing and statistique : {}\".format((time.time() - ti)/60))#collection_term_freq_xml(new_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf01522b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199, 188, 356, 894, 1941, 742]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'bdy_sec.8_NaN' ==> bdy sec.8 NaN\n",
    "# 'bdy_NaN' ==> bdy NaN\n",
    "stat['n_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eca24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "def smart_ltn_xml(posting_list,stat):\n",
    "    i = 0\n",
    "    n = stat['n_element']\n",
    "    df_t = stat['colec_freq']\n",
    "    df = 0\n",
    "    #(1+LOG(tf))*LOG(n/df)\n",
    "    #tf is the frequence of term on a document\n",
    "    #df number of document where the term appear\n",
    "    ltn_value_sec = {} # key sec, value (terme, ltn)\n",
    "    ltn_value = {} # key doc number, value (terme, ltn)\n",
    "    \n",
    "    for term, articles in posting_list.items():\n",
    "        i+=1\n",
    "        dl = df_t[term]\n",
    "        df = dl[-1] # nombre de doc dans lequel le terme apparait sur toute la collection\n",
    "        # print(value) [{'10013': [('sec_1', 1), ('sec_12', 1)]},...]\n",
    "        \n",
    "        for sec in articles:\n",
    "            \n",
    "            for doc_no,val in sec.items():\n",
    "                ltn_value_sec.clear()\n",
    "                for v in val:\n",
    "                    tf = v[1]\n",
    "                    sec =v[0]\n",
    "                    ltn = (1+math.log10(tf))*(math.log10(n/df))\n",
    "                    \n",
    "                    ltn_value_sec.setdefault(sec,[]).append((term,round(ltn,4))) # ltn en fonction de la section\n",
    "                ltn_value.setdefault(doc_no,[]).append(ltn_value_sec.copy()) # ltn en fonction des articles\n",
    "    return ltn_value\n",
    "\n",
    "def wf_refractor(wf):\n",
    "    new_wf = {}\n",
    "    wf_refractor = {}\n",
    "    for doc, article in wf.items():\n",
    "        new_wf.clear()\n",
    "        for l in article: \n",
    "            for se, k in l.items():\n",
    "                for i in k:\n",
    "                    new_wf.setdefault(se,[]).append(i)\n",
    "        wf_refractor[doc] = new_wf.copy()\n",
    "    return wf_refractor\n",
    "\n",
    "def xml_w_score(wf=dict()):\n",
    "    score_wt = {}\n",
    "    for doc_no, article in wf.items():\n",
    "        scores = rsv_score(query,article)\n",
    "        if(len(scores)>0):\n",
    "            for sec in scores:\n",
    "                score_wt.setdefault(doc_no,[]).append(sec)\n",
    "    return score_wt\n",
    "\n",
    "def ltc_xml(ltn):\n",
    "    score_ltn = {}\n",
    "    for doc_no, sec_ltn_value in ltn.items():\n",
    "        for value in sec_ltn_value:\n",
    "            res = smart_ltc(sec_ltn_value)\n",
    "            score_ltn[doc_no] = res\n",
    "    return score_ltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c152d59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stat['colec_freq']\n",
    "#ltn = wf_ref(smart_ltn_xml(new_pl,stat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "268a06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "def tf_bloc(nodes,k,b,term_tf,avdl_el):\n",
    "    tf = term_tf\n",
    "    doc_len = nodes\n",
    "    bloc_1 = tf*(k+1)\n",
    "    bloc_2 = doc_len/avdl_el\n",
    "    bloc_3 = k * ((1-b) + b * bloc_2)\n",
    "    bloc_4 = bloc_3+tf\n",
    "    return bloc_1/bloc_4\n",
    "\n",
    "def tf_part_xml(posting_list,dl,n,k,b, cas):\n",
    "    #(tf*(k+1))/(k*((1-b)+b*(dl/avdl))+tf)\n",
    "    #[  1     ]             [   2   ]\n",
    "    #              [      3         ]\n",
    "    #            [         4            ]\n",
    "    \n",
    "    # [title,bdy_n,bdy_p,sec_n,sec_p,ss_p ,j]\n",
    "    dl_ = sum([dl[x][-1] for x in dl.keys()]) # Nombre de words dans tous les documents\n",
    "    n_elements = sum([dl[x][-1] for x in dl.keys()]) # Nombre de words dans tous les éléments\n",
    "    n_title = sum([dl[x][0] for x in dl.keys()]) # Nombre de words dans tous les title\n",
    "    n_bdy_n = sum([dl[x][1] for x in dl.keys()]) # Nombre de words dans tous les body\n",
    "    n_bdy_p = sum([dl[x][2] for x in dl.keys()]) # Nombre de words dans tous les paragraphes des body\n",
    "    n_sec_n = sum([dl[x][3] for x in dl.keys()]) # Nombre de words dans tous les sections\n",
    "    n_sec_p = sum([dl[x][4] for x in dl.keys()]) # Nombre de words dans tous les paragraphes dans section\n",
    "    n_ss_p = sum([dl[x][5] for x in dl.keys()]) # Nombre de words dans tous les paragraphes dans sous section\n",
    "        #dl[term] = [title,bdy_n,bdy_p,sec_n,sec_p,ss_p ,j]\n",
    "    n_node = stat['n_node'] # nombre total des balises of each element [0,1,2,3,4,5]\n",
    "    doc_len = 0\n",
    "    avdl_el = n_elements/n\n",
    "    #print(avdl)\n",
    "    #avdl = 20\n",
    "    tf_part_val = {}\n",
    "    tf_part_val_l = {}\n",
    "    tf_part_val_node = {}\n",
    "    kl = 0\n",
    "    for term, articles in posting_list.items():\n",
    "        kl+=1\n",
    "        for sec in articles:\n",
    "            for doc_no,val in sec.items():\n",
    "                tf_part_val_node.clear()\n",
    "                for v in val:\n",
    "                    tf = v[1]\n",
    "                    sec =v[0]\n",
    "                    part_el = sec.split('_')\n",
    "                    if part_el[-1] == 'title':\n",
    "                        avdl = n_title/n_node[0]\n",
    "                    if part_el[-1] == 'NaN':\n",
    "                        if 'bdy' in part_el[-2]:\n",
    "                            avdl = n_bdy_n/n_node[1]\n",
    "                        if 'sec' in part_el[-2]:\n",
    "                            avdl = n_sec_n/n_node[3]\n",
    "                        \n",
    "                    if 'p' in part_el[-1]:\n",
    "                        if 'bdy' in part_el[-2]:\n",
    "                            avdl = n_bdy_p/n_node[2]\n",
    "                        if 'sec' in part_el[-2]:\n",
    "                            avdl = n_sec_n/n_node[4]\n",
    "                            \n",
    "                        if 'ss' in part_el[-2]:\n",
    "                            avdl = n_sec_n/n_node[5]\n",
    "                    if cas == 1:\n",
    "                        avdl = avdl_el\n",
    "                    # print(sec.split('_')[-1])\n",
    "                    bm25 =  tf_bloc(n_sec_n,k,b,tf,avdl)\n",
    "                    tf_part_val_node.setdefault(sec,[]).append((term,round(bm25,4))) # ltn en fonction de la section\n",
    "                tf_part_val.setdefault(doc_no,[]).append(tf_part_val_node.copy()) # ltn en fonction des articles\n",
    "    return tf_part_val\n",
    "\n",
    "def idf_part_xml(posting_list,df_,n):\n",
    "    #log((n-df+0.5)/(df+0.5))\n",
    "    #    [   1    ] [  2   ]\n",
    "    idf_part_val ={}\n",
    "    idf_part_val_node ={}\n",
    "    for term, articles in posting_list.items():\n",
    "        for sec in articles:\n",
    "            for doc_no,val in sec.items():\n",
    "                idf_part_val_node.clear()\n",
    "                for v in val:\n",
    "                    sec =v[0]\n",
    "                    tf =  df_[term]\n",
    "                    res =  idf_block(tf,n)\n",
    "                    idf_part_val_node.setdefault(sec,[]).append((term,res))\n",
    "                idf_part_val.setdefault(doc_no,[]).append(idf_part_val_node.copy()) \n",
    "    return idf_part_val\n",
    "\n",
    "def idf_block(tf,n):\n",
    "    df = tf\n",
    "    df = df[-1]\n",
    "    bloc_1 = abs(n-df+0.5)\n",
    "    bloc_2 = df + 0.5\n",
    "    return math.log10(bloc_1/bloc_2)\n",
    "\n",
    "def bm25_xml(posting_list, stat,k,b):\n",
    "    tf_part_ = wf_refractor(tf_part_xml(posting_list,stat['colec_freq'],stat['n_element'],k,b,0))\n",
    "    idf_part_ = wf_refractor(idf_part_xml(posting_list,stat['colec_freq'], stat['n_element']))\n",
    "    bm25_val = {}\n",
    "    # doc 1 : ('a' , tf_part_[a]*idf[a])\n",
    "    for doc, tf_value in tf_part_.items():\n",
    "        for tf in tf_value:\n",
    "            res = [(tf[0],tf[1] * idf[1]) for tf, idf in zip(tf_part_[doc][tf], idf_part_[doc][tf]) if tf[0]==idf[0]]\n",
    "            bm25_val.setdefault(doc, []).append({tf:res})\n",
    "    return bm25_val\n",
    "\n",
    "# Part of weigting function\n",
    "def weinting_function_xml(pl, stat,k,b):\n",
    "    ltn = wf_refractor(smart_ltn_xml(pl,stat))\n",
    "    ltc = ltc_xml(ltn)\n",
    "    bm25_val = wf_refractor(bm25_xml(pl,stat,k,b))\n",
    "    return ltn,ltc,bm25_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30fe4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_part_xml(new_pl,stat['colec_freq'],stat['n_element'],k,b,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "def score_xml(query_list,rsv_wf):\n",
    "    team_name = \"DjibrilMohamedOmaimaDouae\"\n",
    "    score = []\n",
    "    for query in query_list:\n",
    "        query = query.split(' ', 1)\n",
    "        result = result_querys(query[0],rsv_score_(query[1], rsv_wf), team_name)\n",
    "        score.append(result)\n",
    "    return score\n",
    "\n",
    "def result_querys(num_query,rsv_result,team_name):\n",
    "    start = time.time()\n",
    "    #score = reverse_score(rsv_result)\n",
    "    rsv_r = []\n",
    "    k = 0\n",
    "    x = rsv_result\n",
    "    if len(x)>1500:\n",
    "        x = x[:1500]\n",
    "    \n",
    "    # bdy_sec.13_p.2 new.join(s.rsplit(old, maxreplace))\n",
    "    for i in range(len(x)):\n",
    "        path = \"/article[1]/\"\n",
    "        k=0\n",
    "        for el in x[i][1].split(\"_\"):\n",
    "            k+=1\n",
    "            if \"bdy\" in el:\n",
    "                path+= \"bdy[1]/\"\n",
    "            elif \"title\" in el:\n",
    "                path+= \"title[1]/\"\n",
    "            elif \"NaN\" in el:\n",
    "                path = path\n",
    "            else:\n",
    "                ele = el.split('.')\n",
    "                path +=str(\"{}[{}]/\".format(ele[0],ele[1]))\n",
    "        path =''.join(path.rsplit('/',1))\n",
    "        rsv_r.append(str('{} Q0 {} {} {} {} {}'.format(num_query,\n",
    "                                                           x[i][2],i+1,\n",
    "                                                           round(x[i][0],5),\n",
    "                                                           team_name,path))) \n",
    "             \n",
    "    return rsv_r\n",
    "\n",
    "\n",
    "def rsv_score_(query, wf):\n",
    "    rsv_doc = {}\n",
    "    rsv_sec= {}\n",
    "    rsv = []\n",
    "    for term in query.split():\n",
    "        for doc, sec in wf.items():\n",
    "            rsv_sec.clear()\n",
    "            for k,val in sec.items():\n",
    "                for v in val:\n",
    "                    if term == v[0]:\n",
    "                        if k in rsv_sec.keys():\n",
    "                            v_temp = rsv_sec[k] + v[1]\n",
    "                            v[1] = v_temp\n",
    "                        rsv.append((v[1],k,doc))\n",
    "                    else:\n",
    "                        continue \n",
    "            if(len(rsv_sec.keys())>0):\n",
    "                rsv_doc[doc] = rsv_sec.copy()\n",
    "    rsv = sorted(rsv, key=lambda item: item[0],reverse=True)\n",
    "    return rsv\n",
    "\n",
    "# For all weigting function\n",
    "def all_score_wf(query_list, ltn,ltc,bm25):\n",
    "    start = time.time()\n",
    "    score_ltn= score_xml(query,ltn)\n",
    "    print(\"Ltn time : \",time.time() -start)\n",
    "    start = time.time()\n",
    "    score_ltc = score_xml(query,ltc)\n",
    "    print(\"Ltc time : \",time.time() -start)\n",
    "    start = time.time()\n",
    "    score_bm25 = score_xml(query,bm25)\n",
    "    print(\"Bm25 time : \",time.time() -start)\n",
    "    \n",
    "    return score_ltn,score_ltc,score_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a49558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "ltn,ltc,bm25 =weinting_function_xml(new_pl, stat, k, b)\n",
    "score_ltn,score_ltc,score_bm25 = all_score_wf(query, ltn,ltc,bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054e7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one score here and give it to his 2nd paraeter\n",
    "#bm25_s = scores(query,bm25)    |here|\n",
    "run_id = build_run_file(run_id, score_ltn, 0, use_stem, use_stopword, k, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tuning(run_id,score_bm25,use_stem,use_stopword):\n",
    "    start = time.time()\n",
    "    k = 1.2\n",
    "    b = 0.0\n",
    "    for i in range(11):\n",
    "        run_id = build_run_file(run_id, score_bm25,2,use_stem,use_stopword,k,b)\n",
    "        b+=0.1\n",
    "    k = 0.0\n",
    "    b = 0.75\n",
    "    for i in range(21):\n",
    "        run_id = build_run_file(run_id, score_bm25,2,use_stem,use_stopword,k,b)\n",
    "        k+=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115db92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb7aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}