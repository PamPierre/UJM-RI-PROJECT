{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f92839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for imports\n",
    "import re\n",
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4ab9f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cell of all my function\n",
    "\n",
    "def list_file_data(nom_directory):\n",
    "    myListFile = []\n",
    "    for f in os.listdir(nom_directory):\n",
    "        if os.path.isdir(f): # si f est un dossier\n",
    "            os.chdir(f) # On va lister son contenu\n",
    "            parse()\n",
    "            os.chdir('../') # On revient au répertoire précédent\n",
    "        else:\n",
    "            myListFile.append(f)\n",
    "        # Traitement sur le fichier f\n",
    "    return myListFile\n",
    "\n",
    "\n",
    "def preprocesDataFile(fileName):\n",
    "    input_file = gzip.open(fileName, 'rb')\n",
    "    full_text = input_file.read()\n",
    "    docListNum = re.findall('<doc><docno>(.*?)</docno>(.*?)</doc>', str(full_text).lower().strip())\n",
    "    list_doc = re.findall('<doc><docno>(.*?)</docno>', str(full_text).strip())\n",
    "    return docListNum, list_doc\n",
    "\n",
    "def clean(text1,remove_w,stw ):\n",
    "    full_text = text1.replace('\\\\n', '').strip(remove_w).replace(\"''\",' ').replace(\"'\",' ')\n",
    "    full_text = [stemmer.stem(word) \n",
    "                 for word in full_text.split() \n",
    "                 if len(word)>1 \n",
    "                 and word not in stw \n",
    "                 and not word.isnumeric()] ### remove stops_word and applique stemming\n",
    "    #full_text = [word for word in full_text.split() if len(word)>1 and word not in stw and not word.isnumeric()] ### remove word and applique stemming\n",
    "    return ' '.join(full_text)\n",
    "\n",
    "def countWord(words):   \n",
    "    word_count = {} # compte l'occurance d'un terme dans tous les documents\n",
    "    j=0\n",
    "    for word in words: # On parcours la listes de mots\n",
    "        j+=1\n",
    "        word  = word.lower()\n",
    "        if (len(word)==1) or str(word).isnumeric():\n",
    "            continue\n",
    "        if not word in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] = word_count[word] + 1\n",
    "    return word_count\n",
    "\n",
    "def countWordIntoDocs(dico, docno, posting):\n",
    "    docname = docno\n",
    "    \n",
    "    for word, frequence in dico.items():\n",
    "        posting.setdefault(word,[]).append((docname,frequence)) ### Remplace les lignes de commande suivante:\n",
    "        \"\"\"\n",
    "        if not word in list(posting.keys()):\n",
    "            posting[word] = [(docname, frequence)]\n",
    "        else:\n",
    "            posting[word].append((docname, frequence))\n",
    "        \"\"\"\n",
    "    return posting\n",
    "\n",
    "def document_lenght(list_terms,posting_list):\n",
    "    dl = {}\n",
    "    print(\"************\")\n",
    "    print(list_terms.keys())\n",
    "    for doc, value in list_terms.items():\n",
    "        somme_tf = 0\n",
    "        for term in value:\n",
    "            if term.isnumeric():\n",
    "                continue\n",
    "            for val in sorted(posting_list[term]):\n",
    "                if val[0]==doc:\n",
    "                    somme_tf+=val[1]\n",
    "        dl[doc]=somme_tf\n",
    "        return dl\n",
    "    \n",
    "def vocabulary_size(posting_list):\n",
    "    return len(posting_list.keys())\n",
    "\n",
    "\n",
    "def term_len(posting_list):\n",
    "    return [(i, len(i)) for i in posting_list.keys()]\n",
    "\n",
    "def collection_term_freq(posting_list):\n",
    "    c_size={}\n",
    "    dl = {}\n",
    "    for k,v in posting_list.items(): # v = [(doc1,f),(doc2,f)]\n",
    "        somme=0\n",
    "        for _,f in v:\n",
    "            somme+=f\n",
    "        dl[k]=[(somme,len(v))]\n",
    "    return dl\n",
    "        \n",
    "## Pour avoir une courbe avec le plot\n",
    "def plot_datas(data, title, label_x, label_y):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    for key, values in data.items():\n",
    "        x.append(key)\n",
    "        y.append(values[0])\n",
    "    plt.plot(x,y, color='blue',marker='o',linestyle='solid')\n",
    "    #Titre\n",
    "    plt.title(title)\n",
    "    # label\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(label_y)\n",
    "    plt.show()\n",
    "    \n",
    "## Fonction de traintement du texte  \n",
    "def text_mining(fileName):\n",
    "    start = time.time()\n",
    "    docListNum, list_doc = preprocesDataFile(fileName)\n",
    "    posting_list = {}\n",
    "    file_number = fileName.split('/')[1].split('-',1)\n",
    "    dl = list()\n",
    "    for i in range(len(list_doc)):\n",
    "        text_clean = clean(docListNum[i][1], list_char, stopwords)\n",
    "        list_terms[list_doc[i]] = text_clean.split() # Here we create a dictionary of Here we create a dictionary of \n",
    "                                                                              # each docments with its terms\n",
    "        lt = text_clean.split()\n",
    "        current_dico = countWord(lt)\n",
    "        posting_list = countWordIntoDocs(current_dico, list_doc[i], posting_list)\n",
    "    \n",
    "    #dl.append((document_lenght(list_terms,posting_list))) \n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    tf = term_len(posting_list)\n",
    "    file_indexing_infos=(round(elapsed,3),tf)\n",
    "    \n",
    "    return file_indexing_infos, posting_list, list_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0279fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell to initialise variable.\n",
    "directory = \"Practice_02_data/\"\n",
    "list_data = list_file_data(directory)\n",
    "list_data = sorted(list_data)\n",
    "list_char = \"_:/!?#^*~&()[]{}';$%|,.-\"\n",
    "posting_list = {}\n",
    "posting_list_global = {}\n",
    "file_indexing_infos = {}\n",
    "dl = {}\n",
    "tf = {}\n",
    "stopwords = get_stop_words('english')\n",
    "list_terms = {}\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba6964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_filename = str(directory+'01-Text_Only-Ascii-Coll-1-10-NoSem.gz')\n",
    "_,pl,lt = text_mining(default_filename)\n",
    "#dl = document_lenght(lt, pl)\n",
    "dl = [(doc_n, len(len_doc)) for doc_n,len_doc in lt.items()]\n",
    "\n",
    "n = len(dl) # Nombre de documents dans la collection\n",
    "c_tf = collection_term_freq(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4fc58476",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def smart_ltn(posting_list,n, c_tf):\n",
    "    #=(1+LOG(tf))*LOG(n/C$10)\n",
    "    ltn = {}\n",
    "    for key, value in posting_list.items():\n",
    "        for v in value:\n",
    "            tf = 1+math.log(v[1])\n",
    "        for v in c_tf[key]:\n",
    "            c_tl = v[1]\n",
    "        if c_tl==0:\n",
    "            ltn[key] = 0\n",
    "        else:\n",
    "            ltn[key] = tf*math.log(n/c_tl)\n",
    "    ltn_sum = sum(ltn.values())\n",
    "    return ltn_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1b03ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16470.75579881305"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_ltn(pl,n,c_tf)\n",
    "#c_tf = collection_term_freq(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "91233002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tf['rape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac5b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
