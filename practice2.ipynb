{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f92839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for imports\n",
    "import re\n",
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab9f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cell of all my function\n",
    "\n",
    "def list_file_data(nom_directory):\n",
    "    myListFile = []\n",
    "    for f in os.listdir(nom_directory):\n",
    "        if os.path.isdir(f): # si f est un dossier\n",
    "            os.chdir(f) # On va lister son contenu\n",
    "            parse()\n",
    "            os.chdir('../') # On revient au répertoire précédent\n",
    "        else:\n",
    "            myListFile.append(f)\n",
    "        # Traitement sur le fichier f\n",
    "    return myListFile\n",
    "\n",
    "\n",
    "def preprocesDataFile(fileName):\n",
    "    input_file = gzip.open(fileName, 'rb')\n",
    "    full_text = input_file.read()\n",
    "    docListNum = re.findall('<doc><docno>(.*?)</docno>(.*?)</doc>', str(full_text).lower().strip())\n",
    "    list_doc = re.findall('<doc><docno>(.*?)</docno>', str(full_text).strip())\n",
    "    return docListNum, list_doc\n",
    "\n",
    "def clean(text1,use_stopword_stemmer):\n",
    "    \n",
    "    remove_w = \"_:/!?#^*~&()[]{}';$%|,.-\"\n",
    "    stopwords = get_stop_words('english')\n",
    "    full_text = text1.lower().replace('\\\\n', '').strip(remove_w).replace(\"''\",' ').replace(\"'\",' ')\n",
    "    full_text = re.sub(r'[^\\w\\s]', '', full_text) # Remove all punctuation\n",
    "    if(use_stopword_stemmer):\n",
    "        full_text = [stemmer.stem(word) \n",
    "                     for word in full_text.split() \n",
    "                     if len(word)>1 \n",
    "                     and word not in stopwords \n",
    "                     and not word.isnumeric() \n",
    "                     and word.isalnum()] ### remove stops_word and applique stemming\n",
    "        #full_text = [word for word in full_text.split() if len(word)>1 and word not in stw and not word.isnumeric()] ### remove word and applique stemming\n",
    "    else:\n",
    "        full_text = [word \n",
    "                     for word in full_text.split() \n",
    "                     if len(word)>1  \n",
    "                     and not word.isnumeric()\n",
    "                     and word.isalnum()] \n",
    "    return ' '.join(full_text)\n",
    "\n",
    "def countWord(words):   \n",
    "    word_count = {} # compte l'occurance d'un terme dans tous les documents\n",
    "    j=0\n",
    "    for word in words: # On parcours la listes de mots\n",
    "        j+=1\n",
    "        word  = word.lower()\n",
    "        if (len(word)==1) or str(word).isnumeric():\n",
    "            continue\n",
    "        if not word in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] = word_count[word] + 1\n",
    "    return word_count\n",
    "\n",
    "def countWordIntoDocs(dico, docno, posting):\n",
    "    docname = docno\n",
    "    \n",
    "    for word, frequence in dico.items():\n",
    "        posting.setdefault(word,[]).append((docname,frequence)) ### Remplace les lignes de commande suivante:\n",
    "        \"\"\"\n",
    "        if not word in list(posting.keys()):\n",
    "            posting[word] = [(docname, frequence)]\n",
    "        else:\n",
    "            posting[word].append((docname, frequence))\n",
    "        \"\"\"\n",
    "    return posting\n",
    "\n",
    "def doc_len(list_terms):\n",
    "    dl = [(doc_n, len(len_doc)) for doc_n,len_doc in list_terms.items()]\n",
    "    return dl\n",
    "\n",
    "### Function to check after\n",
    "def document_lenght(list_terms,posting_list):\n",
    "    dl = {}\n",
    "    for doc, value in list_terms.items():\n",
    "        somme_tf = 0\n",
    "        for term in value:\n",
    "            if term.isnumeric():\n",
    "                continue\n",
    "            for val in sorted(posting_list[term]):\n",
    "                if val[0]==doc:\n",
    "                    somme_tf+=val[1]\n",
    "        dl[doc]=somme_tf\n",
    "        return dl\n",
    "    \n",
    "def vocabulary_size(posting_list):\n",
    "    return len(posting_list.keys())\n",
    "\n",
    "\n",
    "def term_len(posting_list):\n",
    "    # Return \n",
    "    return [(term, len(term)) for term in posting_list.keys()]\n",
    "\n",
    "\n",
    "def collection_term_freq(posting_list):\n",
    "    c_size={}\n",
    "    dl = {}\n",
    "    for term,values in posting_list.items(): # get the term\n",
    "        somme=0\n",
    "        for v in values:\n",
    "            somme+=v[1]\n",
    "        dl[term]=[(somme,len(values))] #Frequence d'apparution du doc et nombre de doc dans lequel il apparait\n",
    "    return dl\n",
    "\n",
    "## Pour avoir une courbe avec le plot\n",
    "def plot_datas(data, title, label_x, label_y):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    for key, values in data.items():\n",
    "        x.append(key)\n",
    "        y.append(values[0])\n",
    "    plt.plot(x,y, color='blue',marker='o',linestyle='solid')\n",
    "    #Titre\n",
    "    plt.title(title)\n",
    "    # label\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(label_y)\n",
    "    plt.show()\n",
    "    \n",
    "## Fonction de traintement du texte   \n",
    "def text_mining(fileName,use_stopword_stemmer=bool()):\n",
    "    start = time.time()\n",
    "    docListNum, list_doc = preprocesDataFile(fileName)\n",
    "    posting_list = {}\n",
    "    file_number = fileName.split('/')[1].split('-',1)\n",
    "    dl = list()\n",
    "    for i in range(len(list_doc)):\n",
    "        text_clean = clean(docListNum[i][1],use_stopword_stemmer)\n",
    "        list_terms[list_doc[i]] = text_clean.split() # Here we create a dictionary of Here we create a dictionary of \n",
    "                                                                              # each docments with its terms\n",
    "        lt = text_clean.split()\n",
    "        current_dico = countWord(lt)\n",
    "        posting_list = countWordIntoDocs(current_dico, list_doc[i], posting_list)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    tf = term_len(posting_list)\n",
    "    file_indexing_infos=(round(elapsed,3),tf)\n",
    "    return posting_list, list_terms\n",
    "\n",
    "\n",
    "\n",
    "def get_statistics(posting_list,list_terms):\n",
    "    stat = {}\n",
    "    # document length\n",
    "    stat['df'] = doc_len(list_terms)\n",
    "    # Number of a doc\n",
    "    stat['n_doc'] = len(stat['df'])\n",
    "    # term length\n",
    "    stat['tl'] = term_len(posting_list)\n",
    "    #vocabulary size\n",
    "    stat['voc_size'] = vocabulary_size(posting_list)\n",
    "    #collection frequency of terms\n",
    "    stat['colec_freq'] = collection_term_freq(posting_list)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0279fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell to initialise variable.\n",
    "directory = \"Practice_02_data/\"\n",
    "list_data = list_file_data(directory)\n",
    "list_data = sorted(list_data)\n",
    "\n",
    "posting_list = {}\n",
    "posting_list_global = {}\n",
    "file_indexing_infos = {}\n",
    "dl = {}\n",
    "tf = {}\n",
    "list_char = \"_:/!?#^*~&()[]{}';$%|,.-\"\n",
    "stopwords = get_stop_words('english')\n",
    "list_terms = {}\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_filename = str(directory+'01-Text_Only-Ascii-Coll-1-10-NoSem.gz')\n",
    "pl,lt = text_mining(default_filename,True)\n",
    "stat = get_statistics(pl,lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc58476",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1000', 0.0011535997459890008)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"web ranking scoring algorithm\"\n",
    "query_clean = clean(query, True)\n",
    "c_tf = collection_term_freq(pl)\n",
    "\n",
    " \n",
    "def scoring_doc(query_clean, pl, lt):\n",
    "    rsv = {}\n",
    "    stat = get_statistics(pl,lt)\n",
    "    for q in query_clean.split():\n",
    "        if q in pl.keys():\n",
    "            #tf = [f[1] for f in pl[q]]\n",
    "            for v in pl[q]:\n",
    "                tf = (v[1]/len(lt[v[0]]))\n",
    "                idf = math.log(stat['n_doc'] / len(pl[q]))\n",
    "                #val = (1+math.log(value[1]))/math.log(stat['n_doc']/tf)\n",
    "                rsv.setdefault(v[0],[]).append((tf*idf))\n",
    "    rsv = [(doc,sum(v)) for doc,v in rsv.items()]\n",
    "    return rsv\n",
    "\n",
    "rsv = scoring_doc(query_clean, pl, lt)\n",
    "rsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cefa7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_ltn(posting_list,n, c_tf):\n",
    "    #=(1+LOG(tf))*LOG(n/c_tl)\n",
    "    ltn = {}\n",
    "    for key, value in posting_list.items():\n",
    "        for v in value:\n",
    "            tf = 1+math.log(v[1])\n",
    "        for v in c_tf[key]:\n",
    "            c_tl = v[1]\n",
    "        if c_tl==0:\n",
    "            ltn[key] = 0\n",
    "        else:\n",
    "            ltn[key] = tf*math.log(n/c_tl)\n",
    "    ltn_sum = sum(ltn.values())\n",
    "    return ltn_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be372c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1000', 1.052011492633005)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsv = {} \n",
    "for q in query_clean.split():\n",
    "    if q in pl.keys():\n",
    "        tf = sum([f[1] for f in pl[q]])\n",
    "        for value in pl[q]:\n",
    "            val = (1+math.log(value[1]))/math.log(stat['n_doc']/tf)\n",
    "            rsv.setdefault(value[0],[]).append((val))\n",
    "rsv = [(doc,sum(v)) for doc,v in rsv.items()]\n",
    "rsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8c7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
